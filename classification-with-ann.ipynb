{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2170465,"sourceType":"datasetVersion","datasetId":1165452}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/ncikabak/classification-with-ann?scriptVersionId=203042620\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"#   **CLASSİFİCATİON WİTH ANN**\n","metadata":{}},{"cell_type":"markdown","source":" **import libraries**\n**************************************************************************************************************\n**1- import pandas as pd**\n\nPandas is a powerful library for data manipulation and analysis, especially with tabular data (DataFrames). It provides tools for loading, cleaning, filtering, and analyzing data, commonly from CSV and Excel files.\n\npd is a shorthand used for convenience when accessing its functions\n**************************************************************************************************************\n\n**2- import numpy as np**\n\nNumPy is essential for numerical computing, providing support for large multi-dimensional arrays and matrices. It facilitates mathematical operations, linear algebra, and vector manipulations.\n\nnp serves as an abbreviation for quicker access to its functionalities\n**************************************************************************************************************\n**3- import os**\n\nThe OS module allows interaction with the operating system, enabling file and directory management tasks, such as checking file existence and navigating directories.\n\n\n\n\n\n\n\n\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd \nimport numpy as np\nimport os \n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:42:16.55308Z","iopub.execute_input":"2024-10-24T11:42:16.553654Z","iopub.status.idle":"2024-10-24T11:42:16.560493Z","shell.execute_reply.started":"2024-10-24T11:42:16.553601Z","shell.execute_reply":"2024-10-24T11:42:16.558939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1- **label:** Creates an empty list to store the class labels of the images.\n\n  **path:** Creates an empty list to store the file paths of the images.\n\n   *label = []\n    path = []*\n\n************************************************************************************************************\n2- **fish_dir:** Specifies the path to the directory containing the fish dataset.\n\n   os.walk(): Used to navigate through the subdirectories and files in the specified directory.\n\n   Filters for image files with a **.png** extension and excludes files from the GT (Ground Truth) directory.\n*************************************************************************************************************\n\n3- Adds the label (directory name) and the file path of the current image to their respective lists.\n\n\n  *label.append(os.path.split(dir_name)[-1])*\n\n  *path.append(os.path.join(dir_name,filename))*\n***********************************************************************************************************\n\n4- **data:** Creates a pandas DataFrame with two columns: **path** and **label**, organizing the image paths and their      corresponding labels neatly.\n\n  *data = pd.DataFrame(columns=['path','label'])*\n\n  *data['path'] = path*\n\n  *data['label'] = label*\n\n","metadata":{}},{"cell_type":"code","source":"label = []\npath = []\nfish_dir = '/kaggle/input/a-large-scale-fish-dataset/Fish_Dataset/Fish_Dataset'\nfor dir_name, _, filenames in os.walk(fish_dir):\n    for filename in filenames:\n        if os.path.splitext(filename)[-1]=='.png':\n            if dir_name.split()[-1]!='GT':\n                label.append(os.path.split(dir_name)[-1])\n                path.append(os.path.join(dir_name,filename))\n\ndata = pd.DataFrame(columns=['path','label'])\ndata['path']=path\ndata['label']=label\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:42:21.171262Z","iopub.execute_input":"2024-10-24T11:42:21.172093Z","iopub.status.idle":"2024-10-24T11:42:21.359677Z","shell.execute_reply.started":"2024-10-24T11:42:21.172045Z","shell.execute_reply":"2024-10-24T11:42:21.35839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**data.head():**   Provides a quick snapshot of the first few rows of the DataFrame, allowing you to see the data structure at a glance","metadata":{}},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:42:25.530326Z","iopub.execute_input":"2024-10-24T11:42:25.530816Z","iopub.status.idle":"2024-10-24T11:42:25.543661Z","shell.execute_reply.started":"2024-10-24T11:42:25.530773Z","shell.execute_reply":"2024-10-24T11:42:25.542439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**data.tail():**  Provides a quick look at the last few entries in the DataFrame, allowing you to see how the dataset concludes.","metadata":{}},{"cell_type":"code","source":"data.tail()","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:42:28.251291Z","iopub.execute_input":"2024-10-24T11:42:28.252415Z","iopub.status.idle":"2024-10-24T11:42:28.265597Z","shell.execute_reply.started":"2024-10-24T11:42:28.25236Z","shell.execute_reply":"2024-10-24T11:42:28.264137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**data.info():** The method prints information about the DataFrame.\n\n\n**data.describe():** The method returns description of the data in the DataFrame","metadata":{}},{"cell_type":"code","source":"print(data.info())\nprint(data.describe())\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:42:31.527508Z","iopub.execute_input":"2024-10-24T11:42:31.528054Z","iopub.status.idle":"2024-10-24T11:42:31.564547Z","shell.execute_reply.started":"2024-10-24T11:42:31.528004Z","shell.execute_reply":"2024-10-24T11:42:31.563326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**data.isnull():** The method returns a DataFrame object where all the values are replaced with a Boolean value True for NULL values, and otherwise False.","metadata":{}},{"cell_type":"code","source":"print(data.isnull().sum())\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:42:34.44801Z","iopub.execute_input":"2024-10-24T11:42:34.448453Z","iopub.status.idle":"2024-10-24T11:42:34.460358Z","shell.execute_reply.started":"2024-10-24T11:42:34.448414Z","shell.execute_reply":"2024-10-24T11:42:34.458586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**import matplotlib.pyplot as plt:** This imports the pyplot module from the Matplotlib library, which allows for data visualization.\n\n**data['label'].value_counts():** This counts the occurrences of each class (fish species) in the label column. It shows the frequency of each class.\n\n**.plot(kind='bar'):** This plots the class distribution as a bar chart.\n\n**plt.title('Class Distribution'):** This adds a title to the plot.\n\n**plt.xlabel('Class'):** This labels the x-axis with the class names (fish species).\n\n**plt.ylabel('Frequency'):** This labels the y-axis with the frequency counts.\n\n**plt.show():** This displays the plot on the screen.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n\ndata['label'].value_counts().plot(kind='bar')\nplt.title('Class Distribution')\nplt.xlabel('Class')\nplt.ylabel('Frequency')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:42:37.343491Z","iopub.execute_input":"2024-10-24T11:42:37.343963Z","iopub.status.idle":"2024-10-24T11:42:37.610004Z","shell.execute_reply.started":"2024-10-24T11:42:37.343922Z","shell.execute_reply":"2024-10-24T11:42:37.608767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**import cv2:** This imports the OpenCV library for image processing.\n\n**import matplotlib.pyplot as plt:** This imports the pyplot module from Matplotlib for creating visualizations.\n************************************************************************************************************************\n\n**samples_per_class = 3:** This sets the number of sample images to display for each class (in this case, 3 images).\n\n**classes = data['label'].unique():** This retrieves the unique class labels from the label column of the DataFrame.\n\n**n_classes = len(classes):** This calculates the number of classes (assumed to be 9 based on your dataset).\n\n**cols = samples_per_class:** Sets the number of columns in the subplot grid to the defined samples_per_class.\n\n**rows = n_classes:** Sets the number of rows to the number of classes.\n\n**fig, axes = plt.subplots(rows, cols, figsize=(15, rows * 5)):** This creates a grid of subplots with the specified number of rows and columns and sets the figure size.\n\n**axes = axes.flatten():** This flattens the 2D array of axes into a 1D array for easier indexing.\n************************************************************************************************************************\n**sample_images = data[data['label'] == class_name]['path'].sample(...):** This randomly selects sample image paths for the current class, ensuring that the number of samples does not exceed the available images.\n\n**img = cv2.imread(img_path):** Reads the image from the specified path using OpenCV.\n\n**img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB):** Converts the image color format from BGR (OpenCV default) to RGB (for correct color representation in Matplotlib).\n\n**axes[i * cols + j].imshow(img):** Displays the image in the appropriate subplot.\n\n**axes[i * cols + j].set_title(class_name):** Sets the title of the subplot to the class name.\n\n**axes[i * cols + j].axis('off'):** Hides the axis for a cleaner look.\n************************************************************************************************************************\n**plt.tight_layout():** Adjusts the subplot parameters to give specified padding, preventing overlap.\n\n**plt.show():** Displays the final plot with the sampled images.\n\n","metadata":{}},{"cell_type":"code","source":"import cv2\nimport matplotlib.pyplot as plt\n\n\nsamples_per_class = 3\n\n\nclasses = data['label'].unique()\n\n\nn_classes = len(classes)  \ncols = samples_per_class\nrows = n_classes  \n\n\nfig, axes = plt.subplots(rows, cols, figsize=(15, rows * 5))\naxes = axes.flatten()  \n\nfor i, class_name in enumerate(classes):\n    sample_images = data[data['label'] == class_name]['path'].sample(n=min(samples_per_class, len(data[data['label'] == class_name])), random_state=1)\n    \n    for j, img_path in enumerate(sample_images):\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  \n        axes[i * cols + j].imshow(img)\n        axes[i * cols + j].set_title(class_name)\n        axes[i * cols + j].axis('off')\n    \n  \n    for j in range(len(sample_images), cols):\n        axes[i * cols + j].axis('off')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:42:42.296133Z","iopub.execute_input":"2024-10-24T11:42:42.296593Z","iopub.status.idle":"2024-10-24T11:42:50.412703Z","shell.execute_reply.started":"2024-10-24T11:42:42.296553Z","shell.execute_reply":"2024-10-24T11:42:50.410722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**from tensorflow.keras.preprocessing import image:** Imports image processing tools from the Keras library.\n\n**image_data = []:** Creates an empty list to store image data.\n\n**for img_path in data['path']::** Iterates over each image path in the data DataFrame.\n\n**img = image.load_img(img_path, target_size=(64, 64)):** Loads the image from the specified path and resizes it to a fixed size (64x64 pixels).\n\n**img_array = image.img_to_array(img):** Converts the loaded image into a numerical array (numpy array).\n\n**img_array = img_array / 255.0:** Normalizes pixel values to the range of 0-1. This helps the deep learning models perform better.\n\n**img_array = img_array.flatten():** Flattens the image into a one-dimensional array. This makes it suitable for input into an artificial neural network (ANN).\n\n**image_data = np.array(image_data):** Converts the list into a numpy array, preparing it for further processing and analysis.\n","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing import image\n\n\n\nimage_data = []\n\nfor img_path in data['path']:\n  \n    img = image.load_img(img_path, target_size=(64, 64))  # Sabit boyut, örn: 64x64\n\n   \n    img_array = image.img_to_array(img)\n\n   \n    img_array = img_array / 255.0\n\n \n    img_array = img_array.flatten()\n\n    image_data.append(img_array)\n\n\nimage_data = np.array(image_data)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:43:15.171047Z","iopub.execute_input":"2024-10-24T11:43:15.171516Z","iopub.status.idle":"2024-10-24T11:45:05.024011Z","shell.execute_reply.started":"2024-10-24T11:43:15.171473Z","shell.execute_reply":"2024-10-24T11:45:05.022757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**from sklearn.preprocessing import LabelBinarizer:** Imports the LabelBinarizer class from the sklearn.preprocessing module, which is used for converting categorical labels into a one-hot encoded format.\n\n**lb = LabelBinarizer():** Creates an instance of the LabelBinarizer class.\n\n**labels_encoded = lb.fit_transform(data['label']):** Fits the LabelBinarizer to the labels in the data['label'] column and transforms them into a one-hot encoded format. Each unique label will be represented as a binary vector, where 1 indicates the presence of the label and 0 indicates its absence.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelBinarizer\n\n\nlb = LabelBinarizer()\nlabels_encoded = lb.fit_transform(data['label'])\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:46:58.796809Z","iopub.execute_input":"2024-10-24T11:46:58.797445Z","iopub.status.idle":"2024-10-24T11:46:58.874041Z","shell.execute_reply.started":"2024-10-24T11:46:58.7974Z","shell.execute_reply":"2024-10-24T11:46:58.872644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**from sklearn.model_selection import train_test_split:** Imports the train_test_split function from the sklearn.model_selection module. This function is used to split the dataset into training and testing subsets.\n\n**image_data:** The input features (flattened images).\n\n**labels_encoded:** The one-hot encoded labels.\n\n**test_size=0.2:** Specifies that 20% of the dataset should be reserved for testing, while 80% will be used for training.\n\n**random_state=42:** Sets a seed for the random number generator, ensuring that the split is reproducible. Using the same random state will yield the same split each time the code is run.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(image_data, labels_encoded, test_size=0.2, random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:47:03.080203Z","iopub.execute_input":"2024-10-24T11:47:03.08069Z","iopub.status.idle":"2024-10-24T11:47:03.24473Z","shell.execute_reply.started":"2024-10-24T11:47:03.080643Z","shell.execute_reply":"2024-10-24T11:47:03.243257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**import tensorflow as tf:** Imports the TensorFlow library for building and training the neural network.\n\n**from sklearn.model_selection import train_test_split:** Imports the train_test_split function to split the dataset into training and testing sets.\n\n**from tensorflow.keras.models import Sequential:** Imports the Sequential model type from Keras to create a linear stack of layers.\n\n**from tensorflow.keras.layers import Dense, Input:** Imports the Dense and Input layers for building the neural network.\n************************************************************************************************************************\n**Splitting the Dataset**\n\n**X_train, X_test, y_train, y_test = train_test_split(image_data, labels_encoded, test_size=0.2, random_state=42):** Splits the dataset into training (80%) and testing (20%) sets.\n************************************************************************************************************************\n**Creating the Model**\n\n**model = Sequential():** Initializes a new sequential model.\n\n**model.add(Input(shape=(64*64*3,))):** Adds the input layer, specifying the shape of the input data (flattened image size).\n\n**model.add(Dense(128, activation='relu')):** Adds a dense (fully connected) layer with 128 neurons and ReLU activation function.\n\n**model.add(Dense(64, activation='relu')):** Adds another dense layer with 64 neurons and ReLU activation function.\n\n**model.add(Dense(len(lb.classes_), activation='softmax')):** Adds the output layer with a number of neurons equal to the number of classes, using the softmax activation function for multi-class classification.\n***********************************************************************************************************************\n**Compiling the Model**\n\n**model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']):** Compiles the model with the Adam optimizer, categorical crossentropy loss function (suitable for multi-class classification), and tracks accuracy as a metric.\n************************************************************************************************************************\n**Training the Model**\n\nhistory = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test)): Trains the model on the training data for 10 epochs and validates it on the testing data. The training history is stored in the history variable.\n************************************************************************************************************************\n**Printing Training Results**\n\nA loop iterates through each epoch, printing the loss and accuracy metrics for both the training and validation sets:\n\n**Loss function:** The loss value for the current epoch.\n\n**Validation loss:** The validation loss for the current epoch.\n\n**Accuracy:** The training accuracy for the current epoch.\n\n**Validation accuracy:** The validation accuracy for the current epoch","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Input\n\n\nX_train, X_test, y_train, y_test = train_test_split(image_data, labels_encoded, test_size=0.2, random_state=42)\n\n\nmodel = Sequential()\nmodel.add(Input(shape=(64*64*3,)))  # İlk katman input olarak tanımlanıyor\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(len(lb.classes_), activation='softmax'))\n\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n\nhistory = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n\n\nfor epoch in range(len(history.history['loss'])):\n    print(f\"Epoch {epoch + 1}\")\n    print(f\"Loss Function: {history.history['loss'][epoch]}\")\n    print(f\"Validation Loss : {history.history['val_loss'][epoch]}\")\n    print(f\"Accuracy: {history.history['accuracy'][epoch]}\")\n    print(f\"Validation Accuracy: {history.history['val_accuracy'][epoch]}\")\n    print(\"-\" * 50)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:47:07.395213Z","iopub.execute_input":"2024-10-24T11:47:07.395676Z","iopub.status.idle":"2024-10-24T11:47:41.396491Z","shell.execute_reply.started":"2024-10-24T11:47:07.395632Z","shell.execute_reply":"2024-10-24T11:47:41.3951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Plotting Loss Values**\n\nThe training loss *(history.history['loss'])* is plotted against the number of epochs.\n\nThe validation loss *(history.history['val_loss'])* is also plotted for comparison.\n\nEach plot is labeled accordingly.\n\nThe x-axis is labeled as \"Epoch,\" while the y-axis is labeled as \"Loss.\"","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n\nplt.figure(figsize=(12, 6))\nplt.plot(history.history['loss'], label='Training loss)')\nplt.plot(history.history['val_loss'], label='Validation loss)')\nplt.title('Loss Function')\nplt.xlabel('Epoch')\nplt.ylabel('Kayıp (Loss)')\nplt.legend()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:48:03.221344Z","iopub.execute_input":"2024-10-24T11:48:03.221844Z","iopub.status.idle":"2024-10-24T11:48:03.534588Z","shell.execute_reply.started":"2024-10-24T11:48:03.221799Z","shell.execute_reply":"2024-10-24T11:48:03.533411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report\nimport seaborn as sns\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:48:09.272167Z","iopub.execute_input":"2024-10-24T11:48:09.272687Z","iopub.status.idle":"2024-10-24T11:48:09.279602Z","shell.execute_reply.started":"2024-10-24T11:48:09.272641Z","shell.execute_reply":"2024-10-24T11:48:09.277978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Obtaining Predictions**\n\n**model.predict(X_test):** Retrieves the model's predictions for the samples in the test set.\n\n**np.argmax(y_pred, axis=1):** Extracts the classes with the highest probabilities from the prediction results and assigns them to the variable y_pred_classes.\n************************************************************************************************************************\n**Getting True Classes**\n\n**np.argmax(y_test, axis=1):** Obtains the true classes by retrieving the highest probability labels from the y_test array and assigns them to the variable y_true.\n\n**Creating the Confusion Matrix**\n\n**confusion_matrix(y_true, y_pred_classes):** Constructs a matrix that shows the confusion between the true and predicted classes.\n************************************************************************************************************************\n**Visualizing the Confusion Matrix**\n\n**plt.figure(figsize=(10, 7)):** Creates a new figure for the plot.\n\n**sns.heatmap(...):** Uses the Seaborn library to visualize the confusion matrix. annot=True displays the numbers in each cell, fmt='d' shows the values as integers, and cmap='Blues' sets the color palette to shades of blue. xticklabels and yticklabels define the labels for the axes.\n\n**plt.title, plt.xlabel, plt.ylabel:** Sets the title and axis labels for the plot.\n\n**plt.show():** Displays the plot.\n************************************************************************************************************************\n**Generating the Classification Report**\n\n**classification_report(...):** Creates a report that shows the performance metrics (accuracy, precision, recall, F1 score) for the true and predicted classes.\n\n\n","metadata":{}},{"cell_type":"code","source":"y_pred = model.predict(X_test)\ny_pred_classes = np.argmax(y_pred, axis=1)  \n\ny_true = np.argmax(y_test, axis=1)\n\n\ncm = confusion_matrix(y_true, y_pred_classes)\n\n\nplt.figure(figsize=(10, 7))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=lb.classes_, yticklabels=lb.classes_)\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted Class')\nplt.ylabel('Real Class')\nplt.show()\n\n\nreport = classification_report(y_true, y_pred_classes, target_names=lb.classes_)\nprint(\"Classification Report:\\n\", report)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:48:13.093372Z","iopub.execute_input":"2024-10-24T11:48:13.093849Z","iopub.status.idle":"2024-10-24T11:48:14.146205Z","shell.execute_reply.started":"2024-10-24T11:48:13.093804Z","shell.execute_reply":"2024-10-24T11:48:14.144952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Model Creation Function**\n\nThe function create_model(dropout_rate=0.2) initializes a sequential neural network model.\n\n**Input(shape=(X_train.shape[1],)):** Adds an input layer that matches the number of features in the training data.\n\n**Dense(128, activation='relu'):** Adds a fully connected layer with 128 neurons and ReLU activation.\n\n**Dropout(dropout_rate):** Applies dropout with the specified rate to randomly set a fraction of the input units to 0 during training, helping to prevent overfitting.\n\n**Dense(64, activation='relu'):** Adds another fully connected layer with 64 neurons.\n\n**Another Dropout(dropout_rate):** Applies dropout again to further mitigate overfitting.\n\n**Dense(9, activation='softmax'):** Adds the output layer with 9 neurons (one for each class) and softmax activation to output class probabilities.\n\nThe model is compiled with the Adam optimizer and categorical cross-entropy loss.\n************************************************************************************************************************\n**Training with Different Dropout Rates**\n\n**dropout_rates = [0.2, 0.3, 0.4]:** Specifies different dropout rates to be tested.\n\n**histories = []:** Initializes an empty list to store training histories.\n\nCalls create_model(dropout_rate=rate) to create a new model with the current dropout rate.\n\nTrains the model using model.fit(...), specifying validation data and training for 20 epochs.\n\nAppends the dropout rate and training history to the histories list.\n************************************************************************************************************************\n**Visualization of Validation Loss**\n\n**import matplotlib.pyplot as plt:** Imports the Matplotlib library for plotting.\n\nThe second for loop iterates through the stored histories and plots the validation loss for each dropout rate.\n\n**plt.plot(...):** Plots the validation loss for each dropout rate.\n\nSets the plot title, labels, and legend to clarify the results.\n\nFinally, *plt.show()* displays the plot.","metadata":{}},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Input\n\n\ndef create_model(dropout_rate=0.2):\n    model = Sequential()\n    model.add(Input(shape=(X_train.shape[1],)))  \n    model.add(Dense(128, activation='relu'))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(64, activation='relu'))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(9, activation='softmax')) \n    \n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n\ndropout_rates = [0.2, 0.3, 0.4]\nhistories = []\n\nfor rate in dropout_rates:\n    model = create_model(dropout_rate=rate)\n    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=64, verbose=0)\n    histories.append((rate, history))\n\n\nimport matplotlib.pyplot as plt\n\nfor rate, history in histories:\n    plt.plot(history.history['val_loss'], label=f'Dropout Rate: {rate}')\n    \nplt.title('Validation Loss with Different Dropout Rates')\nplt.xlabel('Epochs')\nplt.ylabel('Validation Loss')\nplt.legend()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:48:34.039504Z","iopub.execute_input":"2024-10-24T11:48:34.040179Z","iopub.status.idle":"2024-10-24T11:50:33.682309Z","shell.execute_reply.started":"2024-10-24T11:48:34.040115Z","shell.execute_reply":"2024-10-24T11:50:33.681004Z"},"trusted":true},"execution_count":null,"outputs":[]}]}